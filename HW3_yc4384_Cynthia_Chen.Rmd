---
title: "HW3"
author: 'Yangyang Chen (UNI: yc4384)'
date: "3/25/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---

## Set-Up and Data Preprocessing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(Seurat)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(tidymodels)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(AppliedPredictiveModeling)
library(MASS)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Data Pre-processing
```{r}
set.seed(66)

# Load data, clean column names, eliminate rows containing NA entries
auto_df = read_csv("auto.csv") |>   
  janitor::clean_names() |> 
  na.omit() |> 
  distinct() |> 
  mutate(
    cylinders = as.factor(cylinders),
    year = as.factor(year),
    origin = case_when(origin == "1" ~ "American",
                       origin == "2" ~ "European",
                       origin == "3" ~ "Japanese"),
    origin = as.factor(origin),
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low")
  ) |> 
  as.data.frame()

# Partition data into training/test sets (70% split)
indexTrain = createDataPartition(y = auto_df$mpg_cat,
                                 p = 0.7,
                                 list = FALSE)
```

## Exploratory Data Analysis

```{r}
# Summary statistics
summary(auto_df)
skimr::skim_without_charts(auto_df)
```

We have 392 observations with 8 parameters: 7 predictors, including 4 continuous variables (`displacement`, `horsepower`, `weight`, `acceleration`) and 3 categorical variables (`cylinders`, `year`, `origin`), along with one binary outcome variable, `mpg_cat`, which takes values "high" and "low". Half our observations have the "high" label while the other half have the "low" label.  

```{r}
# Feature plot for all data (training and test), continuous predictors only
theme1 = transparentTheme(trans = 0.4)
trellis.par.set(theme1)

featurePlot(x = auto_df |> dplyr::select(horsepower, displacement, acceleration, weight),
            y = auto_df$mpg_cat,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```

We conduct a few basic exploratory analyses. Our feature plot of continuous covariates shows that cars with high MPG tend to have lower displacement, lower horsepower, lower weight, and higher acceleration. 

## Part (a): Logistic Regression

```{r}
set.seed(2716)

# Logistic regression using the training data (note: not using penalized logistic regression in this case): predict.glm
glm.fit = glm(mpg_cat ~ .,
              data = auto_df,
              subset = indexTrain,
              family = binomial(link = "logit"))

# Check for statistically significant predictors
summary(glm.fit)
```

Here, we build a logistic regression model (without penalization) from our training data. At the 0.05 significance level, `year72`, `year79`, and `year81` are significant predictors of our outcome `mpg_cat`. At the 0.01 significance level, i.e. even more significantly, our indicator variable `year73`, `year82` is a statistically significant predictor of our outcome as well. Other variables are considered as redundent variables.

## Part (b): Model Performance
```{r warning=FALSE, message=FALSE}
# Check performance on test data (use simple classifier with cut-off of 0.5)
test.pred.prob = predict(glm.fit, newdata = auto_df[-indexTrain,],
                           type = "response")

test.pred = rep("low", length(test.pred.prob))

test.pred[test.pred.prob>0.5] = "high"

confusionMatrix(data = as.factor(test.pred),
                reference = auto_df$mpg_cat[-indexTrain],
                positive = "high")
```

Our confusion matrix shows that our accuracy, or overall fraction of correct predictions, is roughly 90% (95% CI: 86% to 96%) once our model is applied to test data. The confusion matrix also tells us that our no information rate is 50%, which means that if we had no information and made the same class prediction for all observations, our model would be 50% accurate. Our p-value near 0 tells us that our accuracy is statistically significantly better than our no information rate. The model' is 87.9% sensitive (true detected positives out of all actual positives) and 93.1% specific (true detected negatives out of all actual negatives), with a positive predictive value of 92.7% (true detected positives out of all predicted positives) and a negative predictive value of 88.5% (true detected negatives out of all predicted negatives). Our sensitivity and specificity average to 90.5%, which is our balanced accuracy. Our kappa, at 0.8103, means that our inter-rater agreement is quite high, even accounting for the possibility of agreement by chance.

## Part (c): MARS Model

```{r}
# Train MARS model using the training data
set.seed(2716)

ctrl = trainControl(method = "repeatedcv",
                    summaryFunction = twoClassSummary,
                    repeats = 5,
                    classProbs = TRUE)

model.mars = train(x = auto_df[indexTrain, 1:7],
                   y = auto_df$mpg_cat[indexTrain],
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3,
                                          nprune = 2:25),
                   metric = "ROC",
                   trControl = ctrl)

summary(model.mars)

ggplot(model.mars, highlight = T)

model.mars$bestTune |> knitr::kable()

coef(model.mars$finalModel) |> knitr::kable(col.names = "Coefficient")

vip(model.mars$finalModel)
```

Overall, our MARS model tells us that `cylinders4` (indicator for having 4 cylinders) is the most important  variable, with continuous variable `displacement` and indicators `year73`, `year72`, and `horsepower` following closely behind, based on the overall impact of each variable on our regression function following a backward elimination procedure. Using `earth`, our model selects 13 out of 27 terms, representing 8 of 22 predictors (nprune terms = 13, product degree = 1). The model is optimized with and has an R-squared of 0.7824.

Importantly, MARS improves the prediction performance compared to logistic regression due to comparatively smaller AIC values (100.4) and deviance (74.3).

## Part (d): LDA

```{r}
# LDA using the training data
lda.fit = lda(mpg_cat ~ ., data = auto_df, subset = indexTrain)

# Increase the bottom margin
par(mar = c(5, 4, 4, 2) + 0.1)  

# Create a new plotting device with custom size
dev.new(width = 10, height = 8)

# Plot the linear discriminants from LDA
plot(lda.fit, col = as.numeric(auto_df$mpg_cat), abbrev = TRUE)

# Obtain scaling matrix
lda.fit$scaling
```

LDA has no tuning parameters, and allows us to classify by nearest centroid. Because we have two classes, we have $k = 2-1 = 1$ linear discriminants, and so our linear discriminant plot gives us the histogram of our transformed X (predictors) for both classes. In this case, when our "X" is lower, we tend to classify in the high `mpg_cat` group, whereas when our "X" is higher, we tend to classify in the low `mpg_cat` group. Finally, the scaling object gives us our matrix A, which is $(k-1) \times p$ matrix, or in this case, a simple column vector with one entry per predictor, given we only have two outcome classes. This matrix allows us to build our x-tilde (which is AX, a product of our transformation matrix and original predictors) for each observation / data point.

```{r}
# Alternatively, use caret for LDA
set.seed(2716)
    
training_df = auto_df[indexTrain, ]

model.lda = train(mpg_cat ~ .,
                  data = training_df,
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)

model.lda$results
```

For completeness, we also run an LDA model using `caret`, which has a 0.97 ROC, with 89.87% sensitivity and 91.59% specificity. 

## Part (e): Model Comparison and AUC/ROC

```{r}
# Model comparison based on ROC (training data)

# Run caret logistic model
set.seed(2132)

glm.logit.caret = train(x = auto_df[indexTrain, 1:7],
                        y = auto_df$mpg_cat[indexTrain],
                        method = "glm",
                        metric = "ROC",
                        trControl = ctrl)

res = resamples(list(LOGISTIC = glm.logit.caret,
                     MARS = model.mars,
                     LDA = model.lda))

summary(res)

bwplot(res, metric = "ROC")
```

Based on resampling / general cross-validation from how our models perform on the training data, having not seen the test data, I would choose the LDA model for classification of our response variable `mpg_cat`, as it has the highest ROC.

```{r}
# Predictions and ROC
lda.predict = predict(model.lda, newdata = auto_df[-indexTrain, 1:7], type = "prob")[,2]

roc.lda = roc(auto_df$mpg_cat[-indexTrain], lda.predict)

# Report AUC and misclassification rate
auc_lda = roc.lda$auc[1]

auc_lda

# Obtain classes
lda_class = lda.predict |> 
  as.data.frame() |> 
  mutate(
    class = case_when(
      lda.predict < 0.50 ~ "low",
      lda.predict > 0.50 ~ "high")
  ) |> 
  dplyr::select(class) |> 
  as.matrix()

# Confusion matrix and misclassification error rate
confusionMatrix(data = as_factor(lda_class),
                reference = auto_df$mpg_cat[-indexTrain],
                positive = "high")

# Plot ROC curve for best model (LDA)
modelName = "LDA model"

pROC::ggroc(list(roc.lda), legacy.axes = TRUE) + 
  scale_color_discrete(labels = paste0(modelName, " (", round(auc_lda, 2),")"),
                       name = "Model Type (AUC)") + 
  geom_abline(intercept = 0, slope = 1, color = "grey")
```

When applied to the previously unseen test data, the LDA model has a misclassification rate of 1 - 0.9052, or ~10%, when we use a threshold of 0.5 probability, as well as an AUC of 0.9753, as observed on our ROC plot above.
